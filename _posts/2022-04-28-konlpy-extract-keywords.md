---
published: true
title:  "[Python] KoNLPy, ì‚¬ì´í‚·ëŸ°ì„ ì´ìš©í•œ ì£¼ìš” ì–´íœ˜ ì¶”ì¶œ"
excerpt: "KoNLPy ì™€ Scikit-learn ì„ ì´ìš©í•´ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì–´íœ˜ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ìğŸ™‹â€â™€ï¸"

categories:
  - About Dev
tags:
  - [NLP ,Python]

toc: true
toc_sticky: true
 
date: 2022-04-29 02:12:08
last_modified_at: 2022-04-29 02:12:12
---

<br>

í…ìŠ¤íŠ¸ì—ì„œ ì£¼íš¨ ì–´íœ˜ë¥¼ ì¶”ì¶œí•´ì•¼ í•  ì¼ì´ ìƒê²¼ë‹¤. ë‹¨ìˆœ ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë§Œ ë”°ì ¸ì„œ ì£¼ìš” ì–´íœ˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì€ ì˜ë¯¸ê°€ ì—†ì„ ê²ƒ ê°™ì•„ ìì—°ì–´ì²˜ë¦¬ ìª½ì„ ì•Œì•„ë³´ì•˜ë‹¤. íŒŒì´ì¬ì— ìœ ìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë§ì•„ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ ë³´ì•˜ë‹¤.

# ğŸ·ï¸ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ê³¼ Scikit-learn

í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì´ë€ ë¹„ì •í˜• í…ŒìŠ¤íŠ¸ì—ì„œ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ íŒ¨í„´, ê´€ê³„ ë“±ì„ ë¶„ì„í•´ ì˜ë¯¸ìˆëŠ” ì •ë³´ë¥¼ ë„ì¶œí•´ë‚´ëŠ” ë°ì´í„° ë§ˆì´ë‹(Mining) ê¸°ë²•ì´ë‹¤. ê¸°ì¡´ì— ë¬¸ìë¡œ êµ¬ì„±ë¼ ìˆë˜ ë°ì´í„° ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ íŠ¹ì§•ì„ ë½‘ì•„ ì–´ë–¤ ê°’ìœ¼ë¡œ ë°”ê¿”ì„œ ìˆ˜ì¹˜í™”í•œë‹¤.

*Scikit-learn(ì‚¬ì´í‚·ëŸ°)*ì€ íŒŒì´ì¬ìš© ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•˜ëŠ” ë° í•„ìš”í•œ ë‹¤ì–‘í•œ ëª¨ë“ˆì„ ì œê³µí•˜ëŠ”ë°, ì˜¤ëŠ˜ì€ ê·¸ì¤‘ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì„ ìœ„í•œ ìì—°ì–´ì²˜ë¦¬ ëª¨ë“ˆë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

**ğŸ“ ìì—°ì–´ íŠ¹ì§• ì¶”ì¶œ Scikit-learn ëª¨ë“ˆ:**
- CountVectorizer: ê° í…ìŠ¤íŠ¸ì—ì„œ íšŸìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•
- TfidfVectorizer: **TF-IDF**ë¼ëŠ” ê°’ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œ
- HashingVectorizer: CounterVectorizerì™€ ì‚¬ìš©ë°©ë²•ì€ ë™ì¼í•˜ì§€ë§Œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•Œ í•´ì‹œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì‹¤í–‰ ì‹œê°„ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŒ. í…ìŠ¤íŠ¸ì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ HashingVectorizerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ

`CountVectorizer` ëŠ” ë‹¨ìˆœ íšŸìˆ˜ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ì¡ê¸° ë•Œë¬¸ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¡°ì‚¬ë‚˜ ê´€ì‚¬, ì§€ì‹œëŒ€ëª…ì‚¬ ë“±ì´ ë†’ì€ íŠ¹ì§•ê°’ì„ ê°€ì ¸ ìœ ì˜ë¯¸í•˜ê²Œ ì‚¬ìš©í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ **TF-IDF**ë¥¼ í™œìš©í•œ `TfidfVectorizer`ì„ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆë‹¤.

ë¨¼ì € TF-IDF ì— ëŒ€í•´ ì•Œì•„ì•¼ í•œë‹¤.

# ğŸ·ï¸ TF-IDF

![image](https://user-images.githubusercontent.com/67352902/165788715-99d28472-f135-4724-9dad-df9cae702772.png){: .align-center}
*TF-IDF*

- TF(Term Frequency): íŠ¹ì • ë‹¨ì–´ê°€ ë°ì´í„°ì—ì„œ ë“±ì¥í•˜ëŠ” íšŸìˆ˜
- DF(Document Frequency): ë¬¸ì„œ ë¹ˆë„ ê°’, íŠ¹ì • ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ëŠ” ì§€í‘œ
- IDF(Inverse Document Frequency): íŠ¹ì • ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë°ì´í„°ì— ë“±ì¥í•˜ì§€ ì•Šì„ìˆ˜ë¡ ê°’ì´ ì»¤ì§
  - ì¡°ì‚¬ë‚˜ ì§€ì‹œëŒ€ëª…ì‚¬ì²˜ëŸ¼ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” TFëŠ” í¬ì§€ë§Œ IDFëŠ” ì‘ì•„ì§€ë¯€ë¡œ CountVectorizerê°€ ê°€ì§„ ë‹¨ì ì„ í•´ê²° í•  ìˆ˜ ìˆë‹¤.

# ğŸ·ï¸ TfidfVectorizer

ì‚¬ì´í‚·ëŸ°ì—ì„  TF-IDFë¥¼ `TfidfVectorizer`ë¡œ ì‰½ê²Œ êµ¬í˜„ í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.

ê° ëª¨ë“ˆë“¤ì˜ ì‚¬ìš©ë²•ì€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šë‹¤. ì‚¬ì´í‚· ëŸ° ê³µì‹ë¬¸ì„œì— ìì„¸í•˜ê²Œ ë‚˜ì™€ìˆë‹¤. ( [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) )

ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•ì€ ì•„ë˜ì™€ ê°™ë‹¤.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pprint

corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?',
]

vectorizer = TfidfVectorizer()
vectorizer.fit(corpus)
matrix = vectorizer.transform(corpus)

pprint.pprint(vectorizer.vocabulary_) # ë‹¨ì–´ ì‚¬ì „
pprint.pprint(matrix.toarray()) # ë¶„ì„ ê²°ê³¼
```

```
# ë‹¨ì–´ ì‚¬ì „
{'and': 0,
 'document': 1,
 'first': 2,
 'is': 3,
 'one': 4,
 'second': 5,
 'the': 6,
 'third': 7,
 'this': 8}

# ë¶„ì„ ê²°ê³¼
array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,
        1.        , 0.38408524, 0.        , 0.38408524],
       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,
        0.53864762, 0.28108867, 0.        , 0.28108867],
       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,
        1.        , 0.26710379, 0.51184851, 0.26710379],
       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,
        1.        , 0.38408524, 0.        , 0.38408524]])
```

`vectorizer.fit(corpus)`ë¥¼ í†µí•´ corpusê°€ ê°€ì§€ê³  ìˆëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ BoWë¡œ êµ¬ì„±í•˜ê³ , ì´ ë‹¨ì–´ë“¤ì— ëŒ€í•´ TF-IDF ê³„ì‚°ì„ í•œ ë’¤ ê° ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ ìœ„ì¹˜ì— TF-IDF ê°’ì´ ë“¤ì–´ê°„ ë²¡í„°ê°€ ë§Œë“¤ì–´ì§„ë‹¤. í‘œí˜„ë˜ì§€ ì•Šì€ ê°’ë“¤ì€ ëª¨ë‘ 0ì´ë‹¤.(í•œê¸€ì ê°™ì€)

## ğŸ”· ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê¸°

ë‹¨ì–´ì™€ valueë¥¼ í•¨ê»˜ ì¶œë ¥í•˜ê³ , ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ì‹¶ë‹¤ë©´ ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•´ ì´ë ‡ê²Œ êµ¬í˜„ í•˜ë©´ ëœë‹¤.

```python
from collections import defaultdict

# ë‹¨ì–´ ì‚¬ì „: {"token": id}
vocabulary_word_id = defaultdict(int)
    
for idx, token in enumerate(vectorizer.get_feature_names()):
    vocabulary_word_id[token] = idx
    
# íŠ¹ì§• ì¶”ì¶œ ê²°ê³¼: {"token": value}
result = defaultdict(str)
    
for token in vectorizer.get_feature_names():
    result[token] = matrix[0, vocabulary_word_id[token]]
    
# ë‚´ë¦¼ì°¨ìˆœ (ì¤‘ìš”ë„ high) ê¸°ì¤€ ì •ë ¬
result = sorted(result.items(), key = lambda item: item[1], reverse = True)
pprint.pprint(result)
```
```python
# ê²°ê³¼
[('first', 0.5802858236844359),
 ('document', 0.46979138557992045),
 ('is', 0.38408524091481483),
 ('the', 0.38408524091481483),
 ('this', 0.38408524091481483),
 ('and', 0.0),
 ('one', 0.0),
 ('second', 0.0),
 ('third', 0.0)]
```

## ğŸ”· íŠ¹ì • tokenizer ì‚¬ìš©

tokenizerë¥¼ ì§€ì •í•˜ì—¬ ì‚¬ìš© í•  ìˆ˜ ìˆë‹¤.

```python
vectorizer = TfidfVectorizer(tokenizer = ì‚¬ìš© í•  í† í¬ë‚˜ì´ì €)
```
ì´ë ‡ê²Œ í•˜ë©´ ë¶„ì„í•  í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì›í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆë‹¤. ì‚¬ìš© í•  í† í¬ë‚˜ì´ì €ë“¤ì€ ì•„ë˜ì— ì†Œê°œí•œë‹¤.

# ğŸ·ï¸ í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬

## ğŸ”· NLTK: ì˜ì–´

ì˜ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‘ì—…ì—” NLTKë¥¼ ë§ì´ ì“´ë‹¤. ì´ ê¸€ì—ì„œëŠ” í•œê¸€ í…ìŠ¤íŠ¸ ë¶„ì„ì„ ì£¼ë¡œ ë‹¤ë£¨ê¸° ë•Œë¬¸ì— ì˜ ì„¤ëª…ë˜ì–´ ìˆëŠ” [ë ˆí¼ëŸ°ìŠ¤](https://mr-doosun.tistory.com/23)ë§Œ ì†Œê°œí•˜ê² ë‹¤.

## ğŸ”· KoNLPy: í•œêµ­ì–´

í•œêµ­ì–´ ì •ë³´ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´ì¬ íŒ¨í‚¤ì§€ì´ë‹¤. ì°¸ê³ ë¡œ 'ì½”ì—”ì—˜íŒŒì´' ë¼ê³  ì½ëŠ”ë‹¤. KoNLPyëŠ” ë‹¤ì–‘í•œ íƒœê¹… íŒ¨í‚¤ì§€ë“¤ì„ ì œê³µí•œë‹¤. [ê³µì‹ë¬¸ì„œ](https://konlpy.org/ko/latest/index.html) ê°€ ì˜ ë˜ì–´ìˆê¸° ë•Œë¬¸ì— ê° íƒœê¹… í´ë˜ìŠ¤ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ë ¤ë©´ ì­‰ ì½ì–´ë³´ëŠ”ê²ƒë„ ì¢‹ë‹¤.

**tag Classes:**
- Hannanum
- Kkma
- Komoran
- Mecab (Macì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥)
- Okt

ì´ì¤‘ Okt ì‚¬ìš©í•´ ë³´ë ¤ê³  í•œë‹¤. í•„ìë„ ì´ ê¸€ì„ ì“°ë‹¤ê°€ ì•Œê²Œ ëœê±´ë° ê¸°ì¡´ì˜ Twitter íŒ¨í‚¤ì§€ëŠ” Oktë¡œ ë³€ê²½ ë˜ì—ˆë‹¤ê³  í•œë‹¤.([#141](https://github.com/konlpy/konlpy/issues/141))

### ğŸ”¸ KoNLPy ì‚¬ìš©ë²•

```python
>>> from konlpy.tag import Okt
>>> okt = Okt()
>>> print(okt.morphs(u'ë‹¨ë…ì…ì°°ë³´ë‹¤ ë³µìˆ˜ì…ì°°ì˜ ê²½ìš°'))
['ë‹¨ë…', 'ì…ì°°', 'ë³´ë‹¤', 'ë³µìˆ˜', 'ì…ì°°', 'ì˜', 'ê²½ìš°']
>>> print(okt.nouns(u'ìœ ì¼í•˜ê²Œ í•­ê³µê¸° ì²´ê³„ ì¢…í•©ê°œë°œ ê²½í—˜ì„ ê°–ê³  ìˆëŠ” KAIëŠ”'))
['í•­ê³µê¸°', 'ì²´ê³„', 'ì¢…í•©', 'ê°œë°œ', 'ê²½í—˜']
>>> print(okt.phrases(u'ë‚ ì¹´ë¡œìš´ ë¶„ì„ê³¼ ì‹ ë¢°ê° ìˆëŠ” ì§„í–‰ìœ¼ë¡œ'))
['ë‚ ì¹´ë¡œìš´ ë¶„ì„', 'ë‚ ì¹´ë¡œìš´ ë¶„ì„ê³¼ ì‹ ë¢°ê°', 'ë‚ ì¹´ë¡œìš´ ë¶„ì„ê³¼ ì‹ ë¢°ê° ìˆëŠ” ì§„í–‰', 'ë¶„ì„', 'ì‹ ë¢°', 'ì§„í–‰']
>>> print(okt.pos(u'ì´ê²ƒë„ ë˜ë‚˜ìš¬ã…‹ã…‹'))
[('ì´', 'Determiner'), ('ê²ƒ', 'Noun'), ('ë„', 'Josa'), ('ë˜ë‚˜ìš¬', 'Noun'), ('ã…‹ã…‹', 'KoreanParticle')]
>>> print(okt.pos(u'ì´ê²ƒë„ ë˜ë‚˜ìš¬ã…‹ã…‹', norm=True))
[('ì´', 'Determiner'), ('ê²ƒ', 'Noun'), ('ë„', 'Josa'), ('ë˜ë‚˜ìš”', 'Verb'), ('ã…‹ã…‹', 'KoreanParticle')]
>>> print(okt.pos(u'ì´ê²ƒë„ ë˜ë‚˜ìš¬ã…‹ã…‹', norm=True, stem=True))
[('ì´', 'Determiner'), ('ê²ƒ', 'Noun'), ('ë„', 'Josa'), ('ë˜ë‹¤', 'Verb'), ('ã…‹ã…‹', 'KoreanParticle')]
```
ì˜ˆì œ ì¶œì²˜: [ê³µì‹ë¬¸ì„œ](https://konlpy.org/ko/latest/api/konlpy.tag/#okt-class)

- `morphs()` : í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤. ì˜µì…˜ìœ¼ë¡œ norm(ë¬¸ì¥ ì •ê·œí™”), stem(ì–´ê°„ ì¶”ì¶œ) ì´ ìˆëŠ”ë°, True/False ê°’ì„ ì£¼ë©´ ëœë‹¤.
- `nouns()` : í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
- `pharases()` : í…ìŠ¤íŠ¸ì—ì„œ ì–´ì ˆì„ ì¶”ì¶œ
- `pos()` : ê° í’ˆì‚¬ë¥¼ íƒœê¹…. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , ê° í˜•íƒœì†Œì— í•´ë‹¹í•˜ëŠ” í’ˆì‚¬ì™€ í•¨ê»˜ ë¦¬ìŠ¤íŠ¸í™” í•œë‹¤. ì˜µì…˜ìœ¼ë¡œ norm, stem, joinì´ ìˆë‹¤.

### ğŸ”¸ TfidfVectorizerì— ì ìš©

#### KoNLPy ì ìš© ì „

ì¢€ ì „ì— [TfidfVectorizer](https://devyuseon.github.io/about%20dev/konlpy-extract-keywords/#tfidfvectorizer) ì—ì„œ ì‚¬ìš©í•œ ì½”ë“œë¥¼ ë³„ë„ì˜ í† í¬ë‚˜ì´ì € ì§€ì • ì—†ì´ ì•„ë˜ corpusë¥¼ ë„£ì–´ì„œ ëŒë ¤ë³´ì•˜ë‹¤.

```python
corpus = [
    'ì² ìˆ˜ëŠ” í†µê³„í•™ê³¼ì— ë‹¤ë‹Œë‹¤.',
    'ë¹…ë°ì´í„° ë¶„ì„ì— í•„ìš”í•œ ê²ƒì€ í†µê³„í•™ì  ì§€ì‹ê³¼ í”„ë¡œê·¸ë˜ë° ëŠ¥ë ¥ì´ë‹¤.'
    '4ì°¨ì‚°ì—…ì˜ í•µì‹¬ê¸°ìˆ ë¡œ ì¸ê³µì§€ëŠ¥ê³¼ ë¹…ë°ì´í„°ê°€ ìˆë‹¤.',
    'í…ìŠ¤íŠ¸ìë£ŒëŠ” ë¹…ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ì¬ë£Œì´ë‹¤.'
]
```
ì˜ˆì œ ì¶œì²˜ : [bigdata.dongguk.ac.kr](http://bigdata.dongguk.ac.kr/lectures/TextMining)

```python
[('ë‹¤ë‹Œë‹¤', 0.5773502691896257),
 ('ì² ìˆ˜ëŠ”', 0.5773502691896257),
 ('í†µê³„í•™ê³¼ì—', 0.5773502691896257),
 # 0ì¸ ê²°ê³¼ëŠ” ìƒëµ
 ]
```

#### KoNLPy ì ìš© í›„

```python
def okt_tokenizer(text):
    okt = Okt()
    return okt.nouns(text)

vectorizer = TfidfVectorizer(tokenizer=okt_tokenizer)
# ìƒëµ
```

```python
[('ì² ìˆ˜', 0.5773502691896257),
 ('í†µê³„', 0.5773502691896257),
 ('í•™ê³¼', 0.5773502691896257),
 # 0ì¸ ê²°ê³¼ëŠ” ìƒëµ
 ]
```
ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë°ì´í„°ì— í›¨ì”¬ ë” ê°€ê¹Œì›Œì¡Œë‹¤.

### ğŸ”¸ íŠ¹ìˆ˜ê¸°í˜¸ ë° ë¶ˆìš©ì–´ ì œê±°

'!,.%$' ì™€ ê°™ì€ íŠ¹ìˆ˜ê¸°í˜¸ë‚˜ ì¡°ì‚¬, ì ‘ì†ì‚¬, ì§€ì‹œëŒ€ëª…ì‚¬ ë“±ì˜ ë¶ˆìš©ì–´ë¥¼ ì¶”ê°€ë¡œ ì œê±°í•´ì•¼ ìœ ì˜ë¯¸í•œ ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

ì˜ì–´ì˜ ê²½ìš° NLTKì—ì„œ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì§€ë§Œ í•œêµ­ì–´ëŠ” ë”°ë¡œ ì œê³µë˜ëŠ” ë¦¬ìŠ¤íŠ¸ê°€ ì—†ë‹¤. ë”°ë¼ì„œ ë”°ë¡œ ì •ì˜í•´ë†“ê³  ì‚¬ìš©í•´ì•¼ í•œë‹¤.

ê³µê°œëœ í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸:
- https://www.ranks.nl/stopwords/korean
- https://bab2min.tistory.com/544

í•„ìëŠ” ê²€ìƒ‰í•˜ë©° ì°¾ì€ ë¶ˆìš©ì–´ë“¤ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ í•©ì³ì„œ ì‚¬ìš©í–ˆë‹¤.
- [ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ í•©ë³¸](https://github.com/2E2I/donation-crawler/blob/main/stopwords.txt)

**í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì†ŒìŠ¤ì½”ë“œ:**

```python
def get_stopwords():
    stopwords = list()
    
    f = open('./stopwords.txt', 'r', encoding='utf-8')
    
    while True:
        line = f.readline()
        if not line: break
        stopwords.append(line.strip())
        
    return stopwords

def okt_tokenizer(text):
    okt = Okt()
    text = re.sub(r'[^ ã„±-ã…£ê°€-í£A-Za-z]', '', text) # íŠ¹ìˆ˜ê¸°í˜¸ ì œê±°
    stopwords = get_stopwords() # ë¶ˆìš©ì–´
    
    return [token for token in okt.nouns(text)
            if len(token) > 1 and token not in stopwords]
```

<br>

ì „ì²´ ì†ŒìŠ¤ì½”ë“œëŠ” [âœ¨ì—¬ê¸°ì„œâœ¨](https://github.com/2E2I/donation-crawler/blob/main/extract.py) í™•ì¸ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:)
<br>
