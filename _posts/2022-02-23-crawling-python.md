---
published: true
title:  "[Python] íŒŒì´ì¬ìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ (Feat. í•´í”¼ë¹ˆ, Selenium, BeautifulSoup4)"
excerpt: "íŒŒì´ì¬ìœ¼ë¡œ ì›¹ í˜ì´ì§€ì˜ ë”ë³´ê¸° ë²„íŠ¼ì„ ëê¹Œì§€ ëˆŒëŸ¬ì„œ ì „ì²´ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§ í•´ë³´ì!"

categories:
  - Database
tags:
  - [Database, ElasticSearch, Python, Selenium, BeautifulSoup4]

toc: true
toc_sticky: true
 
date: 2022-02-23 01:29:39
last_modified_at: 2022-02-23 01:29:36
---

<br>

> í˜„ì¬ ê¸°ë¶€ í”Œë«í¼, ì‚¬ì´íŠ¸ë“¤ì„ í¬ë¡¤ë§ í•´ì™€ì„œ í•œêº¼ë²ˆì— ë³´ì—¬ì£¼ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” í•´í”¼ë¹ˆ ì‚¬ì´íŠ¸ì˜ ìº í˜ì¸ë“¤ì„ í¬ë¡¤ë§í•´ì™€ jsoníŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê³¼ì •ì„ ì†Œê°œí•©ë‹ˆë‹¤. 'ìŠ¤í¬ë˜í•‘'ê³¼ 'í¬ë¡¤ë§'ì´ ì¡°ê¸ˆ ë‹¤ë¥´ì§€ë§Œ ì´ ê¸€ì—ì„œëŠ” í¬ë¡¤ë§ìœ¼ë¡œ í‘œê¸°í–ˆìŠµë‹ˆë‹¤!

[ğŸ«’í•´í”¼ë¹ˆ](https://happybean.naver.com/donation/DonateHomeMain)

# â˜ï¸ ëª©ì ?

![image](https://user-images.githubusercontent.com/67352902/155139609-071e0aee-a07f-401f-ab0e-0895098afa04.png){: .align-center}

í•´í”¼ë¹ˆì—ì„œ 'ê¸°ë¶€'íƒ­ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ ê° ìº í˜ì¸ì´ ì¹´ë“œ í˜•íƒœë¡œ ë‚˜ì—´ë˜ì–´ ìˆê³ , ì¹´ë“œë¥¼ í´ë¦­í•˜ë©´ ì„¸ë¶€ í˜ì´ì§€ë¡œ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤.

ì €ëŠ” ì—¬ê¸°ì„œ ê° ìº í˜ì¸ ì¹´ë“œì˜ urlì„ ì–»ì–´ì„œ í•´ë‹¹ urlì˜ ë‚´ìš©ì„ í¬ë¡¤ë§ í•´ì™€ì•¼ í•©ë‹ˆë‹¤.

# â›… ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° íŒŒì•…

ì‚¬ì´íŠ¸ ìœ„ì—ì„œ `ì˜¤ë¥¸ìª½ ë§ˆìš°ìŠ¤ > ê²€ì‚¬` ë¡œ ë“¤ì–´ê°€ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤(í¬ë¡¬ê³¼ ì—£ì§€/ë‹¤ë¥¸ ì‚¬ì´íŠ¸ëŠ” ì˜ ëª¨ë¦…ë‹ˆë‹¤). ë‚´ê°€ í¬ë¡¤ë§ í•˜ê³ ì í•˜ëŠ” êµ¬ì—­ì„ ì°¾ì•„ divì•ˆìœ¼ë¡œ ê³„ì† ë“¤ì–´ê°€ë´…ë‹ˆë‹¤. ë§ˆìš°ìŠ¤ë¥¼ ìœ„ë¡œ ì˜¬ë¦¬ë©´ êµ¬ì—­ì´ í‘œì‹œë©ë‹ˆë‹¤.

í•´í”¼ë¹ˆì€ idê°€ rdonaboxes, classê°€ card_wrapì¸ div ì•ˆì—
![image](https://user-images.githubusercontent.com/67352902/155140393-0656bfde-50c4-4e7f-b632-5af9cd073995.png){: .align-center}

classê°€ cardì´ê³  ê° ì„¸ë¶€ í˜ì´ì§€ë¡œ ë§í¬ë˜ì–´ìˆëŠ” aíƒœê·¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
![image](https://user-images.githubusercontent.com/67352902/155140632-795f6761-1878-4fc0-a28b-6bb93e932f20.png){: .align-center}

ê° ìº í˜ì¸ë“¤ì€ "H000000183166"ê°™ì€ ê³ ìœ  ì•„ì´ë””ë¥¼ ê°–ê³  ìˆê³  ì„¸ë¶€ í˜ì´ì§€ëŠ”
```
https://happybean.naver.com/donations/ + [ìº í˜ì¸ ì•„ì´ë””]
```
ì´ë ‡ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì €ëŠ” ì „ì²´ ìº í˜ì¸ì„ ëª¨ë‘ í¬ë¡¤ë§ í•´ì™€ì•¼ í•˜ê¸° ë•Œë¬¸ì— [https://happybean.naver.com/donation/DonateHomeMain](https://happybean.naver.com/donation/DonateHomeMain)ì—ì„œ 'ë”ë³´ê¸°' ë²„íŠ¼ì„ ëê¹Œì§€ ëˆŒëŸ¬ ìº í˜ì¸ ì „ì²´ê°€ ë‚˜íƒ€ë‚˜ë„ë¡ í•œ ë‹¤ìŒ í¬ë¡¤ë§ í•´ ì˜¬ ê²ƒì…ë‹ˆë‹¤.

# â›ˆï¸ Selenium ì„¸íŒ…

ì •ì  í˜ì´ì§€ë§Œ í¬ë¡¤ë§(ìŠ¤í¬ë˜í•‘) í•œë‹¤ë©´ BeautifulSoup4ìœ¼ë¡œë„ ì¶©ë¶„í•˜ì§€ë§Œ, ë™ì ìœ¼ë¡œ í˜ì´ì§€ urlì„ ëª¨ì•„ì™€ì•¼ í•˜ë¯€ë¡œ Seleniumì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì™¸ì—ë„ ë¡œê·¸ì¸ì„ í•´ì„œ ë“¤ì•„ê°„ë‹¤ë˜ê°€ í•˜ëŠ” ì‘ì—…ë„ Seleniumìœ¼ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë¨¼ì € í¬ë¡¬ ë“œë¼ì´ë²„ë¥¼ ì„¤ì¹˜í•œ ë’¤ ì§„í–‰í•©ë‹ˆë‹¤. (í¬ë¡¬ ë“œë¼ì´ë²„ë¥¼ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì€ ì´ ê¸€ì— ì†Œê°œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)
  
```python
def set_chrome_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("headless")  # ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë„ìš°ì§€ ì•ŠëŠ” headless chrome ì˜µì…˜
    options.add_argument("lang=ko_KR")  # ì–¸ì–´ ì„¤ì •
    options.add_experimental_option(
        "excludeSwitches", ["enable-logging"]
    )  # ê°œë°œë„êµ¬ ë¡œê·¸ ìˆ¨ê¸°ê¸°
    options.add_argument("start-maximized") # ì°½ í¬ê¸° ìµœëŒ€ë¡œ
    options.add_argument("disable-infobars")
    options.add_argument("--disable-extensions")
    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()), options=options
    )
    return driver

if __name__ == "__main__":    
    data = [] # í¬ë¡¤ë§ ê²°ê³¼ ë“¤ì–´ê°ˆ ë¦¬ìŠ¤íŠ¸

    driver = set_chrome_driver()
    driver.get(_URL_DONATE_LIST)
    # _URL_DONATE_LIST = "https://happybean.naver.com/donation/DonateHomeMain"
```

í•„ìš”í•œ optionì„ ì¶”ê°€í•œ ë’¤ driverë¥¼ ì„¤ì •í•´ì¤ë‹ˆë‹¤.

# ğŸŒ¤ï¸ 'ë”ë³´ê¸°' ë²„íŠ¼ ëê¹Œì§€ í´ë¦­

![image](https://user-images.githubusercontent.com/67352902/155143857-ad4576ed-0dd3-4ae4-93c9-c139301f77ab.png){: .align-center}

ë”ë³´ê¸° ë²„íŠ¼ì˜ idê°€ '#btn_more'ì¸ ê²ƒì„ ì•Œì•„ëƒˆìŠµë‹ˆë‹¤.

ìº í˜ì¸ ìˆ˜ê°€ ì ì€ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì—ì„œ ë”ë³´ê¸° ë²„íŠ¼ì„ ëê¹Œì§€ ëˆ„ë¥´ë©´ ë²„íŠ¼ì˜ ì†ì„±ì´ `display:none`ì´ ë˜ëŠ”ê²ƒì„ í™•ì¸ í–ˆê¸°ë•Œë¬¸ì—, ë²„íŠ¼ì´ noneì´ ë ë•Œê¹Œì§€ ê³„ì† ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ë©´ ë©ë‹ˆë‹¤.

ë²„íŠ¼ì„ í´ë¦­ í•  ìˆ˜ ìˆì„ë•Œê¹Œì§€ 1ì´ˆ ê¸°ë‹¤ë ¤ ì£¼ê²Œ ì„¤ì •í–ˆê³ , ë§Œì•½ 1ì´ˆì•ˆì— í´ë¦­í•˜ì§€ ëª»í•˜ë©´ TimeoutExceptionì´ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤.

```python
# ë”ë³´ê¸° ë²„íŠ¼ ëê¹Œì§€ í´ë¦­
while driver.find_element(By.CSS_SELECTOR, "#btn_more").is_displayed:
    try:
        WebDriverWait(driver, 1).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "#btn_more"))
        ).click()
    except TimeoutException:
        break
```

ì„±ê³µì ìœ¼ë¡œ ë”ë³´ê¸° ë²„íŠ¼ì„ ëê¹Œì§€ ëˆŒë €ë‹¤ë©´, ì´ì œ ëª¨ë“  ìº í˜ì¸ì´ í˜ì´ì§€ì— ë…¸ì¶œë˜ì—ˆì„ ê²ë‹ˆë‹¤.

# ğŸŒ¥ï¸ URL ì¶”ì¶œ

`<a>` íƒœê·¸ë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ” ê° ìº í˜ì¸ ì¹´ë“œì˜ í´ë˜ìŠ¤ëŠ” `card`ì˜€ê¸° ë•Œë¬¸ì— í´ë˜ìŠ¤ê°€ `card`ì¸ elementsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

ì´ì œ ë°˜ë³µë¬¸ì„ ëŒë©´ì„œ ê° ìº í˜ì¸ ì¹´ë“œì— ëŒ€í•œ urlì„ ì–»ê³ , í¬ë¡¤ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤.

`.get_attribute("")`ë¡œ í•´ë‹¹ html íƒœê·¸ì˜ ì†ì„±ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ë°, ì´ ë©”ì†Œë“œë¥¼ ì´ìš©í•´ ìº í˜ì¸ urlê³¼ ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì£¼ì†Œë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.

```python
# ê° ìº í˜ì¸ ì¹´ë“œì— ëŒ€í•œ í¬ë¡¤ë§ ì§„í–‰
html_campaign_cards = driver.find_elements(By.CLASS_NAME, "card")
for card in html_campaign_cards:
    campaign_url = card.get_attribute("href")
    campaign_thumbnail = card.find_element(
        By.CSS_SELECTOR, "a > img"
    ).get_attribute("src")
    crawling_each_campaign(campaign_url, campaign_thumbnail)
driver.close()
```

# ğŸŒ¦ï¸ ì„¸ë¶€ í˜ì´ì§€ í¬ë¡¤ë§

`crawling_each_campaign()` ë¼ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì„œ dataì— ìº í˜ì¸ ê°ì²´ë¥¼ appendí•´ì£¼ë„ë¡ êµ¬í˜„í–ˆëŠ”ë°, ê° ì„¸ë¶€ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§(ìŠ¤í¬ë˜í•‘) í•˜ëŠ”ê²ƒì€ Seleniumì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  **BeautifulSoup4**ì„ ì´ìš© í–ˆìŠµë‹ˆë‹¤.

Seleniumì€ ë™ì  í˜ì´ì§€ë¥¼ í¬ë¡¤ë§ í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ ëŠë¦¬ê¸° ë•Œë¬¸ì—, ì •ì  í˜ì´ì§€ì¸ ì„¸ë¶€í˜ì´ì§€ëŠ” bs4(beautifulSoup4)ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

<details>
<summary>ì„¸ë¶€í˜ì´ì§€ í¬ë¡¤ë§(ìŠ¤í¬ë˜í•‘)í•˜ëŠ” ì†ŒìŠ¤ì½”ë“œ</summary>
<div markdown="1">
```python
class Campaign:
    def __init__(
        self,
        campaign_id,
        title,
        category,
        theme,
        body,
        organization_name,
        thumbnail,
        due_date,
        start_date,
        target_price,
        status_price,
    ):
        self.campaign_id = campaign_id
        self.title = title
        self.category = category
        self.theme = theme
        self.body = body
        self.organization_name = organization_name
        self.thumbnail = thumbnail
        self.due_date = due_date
        self.start_date = start_date
        self.target_price = target_price
        self.status_price = status_price

def get_title(soup: BeautifulSoup):
    return soup.find("h3", "tit").text


def get_theme_and_category(soup: BeautifulSoup):
    _theme = soup.find("a", "theme").text.split(">")
    theme = _theme[1].strip()
    category = _theme[0].strip()
    return theme, category


def get_body(soup: BeautifulSoup):
    body = soup.select_one(
        "#container > div > div.collect_content > div > ul.intro_lst.editor_base"
    ).text.strip()
    return body


def get_organization_name(soup: BeautifulSoup):
    return soup.select_one(
        "#container > div > div.collect_side > div.section_group > div > h3 > span > a > strong"
    ).text.strip()


def get_dates(soup: BeautifulSoup):
    _term = soup.select_one(
        "#container > div > div.collect_side > div.section_status > div.term_area > p > strong"
    ).text.split("~")
    start_date = _term[0].strip()
    due_date = _term[1].strip()
    return start_date, due_date


def get_prices(soup: BeautifulSoup):
    status_price = soup.select_one(
        "#container > div > div.collect_side > div.section_status > div.num_area > p.status_num > strong"
    ).text.replace(",", "")
    target_price = soup.select_one(
        "#container > div > div.collect_side > div.section_status > div.num_area > p.detail_num.v2 > strong > span"
    ).text.replace(",", "")
    return status_price, target_price


def crawling_each_campaign(url: str, src: str):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    campaign_id = url.split("/")[4]
    title = get_title(soup)
    theme, category = get_theme_and_category(soup)
    body = get_body(soup)
    organization_name = get_organization_name(soup)
    thumbnail = src
    start_date, due_date = get_dates(soup)
    status_price, target_price = get_prices(soup)

    campaign = Campaign(
        campaign_id,
        title,
        category,
        theme,
        body,
        organization_name,
        thumbnail,
        due_date,
        start_date,
        target_price,
        status_price,
    )
    data.append(campaign.__dict__)
```
</div>
</details><br>

**ğŸ“ selector ì–»ê¸°**

bs4ë¥¼ ì´ìš©í•˜ì—¬ í¬ë¡¤ë§ í•´ì˜¤ëŠ” ë°©ë²•ì€ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆì–´ ìì„¸í•˜ê²Œ ì–¸ê¸‰ì€ í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤ë§Œ, ìœ„ ì½”ë“œì— `soup.select_one()`ìœ¼ë¡œ í•œë²ˆì— ì°¾ì•„ì™”ëŠ”ë° ì´ê±´ í•˜ë‚˜í•˜ë‚˜ ë‹¤ ì¨ì¤€ê²Œ ì•„ë‹ˆë¼ ê°œë°œìë„êµ¬(ê²€ì‚¬) ì—ì„œ ì¶”ì¶œí•´ ì˜¨ ê²ƒì…ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/67352902/155172889-6a7e096b-b8ed-45d2-8f6f-4367050d8562.png){: .align-center}

ê°œë°œìë„êµ¬ ì°½ì—ì„œ ì›í•˜ëŠ” ê²ƒì„ í´ë¦­ > ë³µì‚¬ > selectorë³µì‚¬ë¥¼ ëˆ„ë¥´ë©´ ì‰½ê²Œ pathë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Seleniumìœ¼ë¡œ í¬ë¡¤ë§ í•  ë•ŒëŠ” XPathë„ ìœ ìš©í•˜ê²Œ ì“°ì¼ ìˆ˜ ìˆê¸°ë•Œë¬¸ì— ì–´ë µê²Œ êµ¬í•˜ì§€ ë§ê³  ì´ë ‡ê²Œ ë³µì‚¬í•˜ë©´ ë©ë‹ˆë‹¤.

**ğŸ“ Object -> ë”•ì…”ë„ˆë¦¬ ë³€í™˜**

jsonìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ê³  í›„ì— ì—˜ë¼ìŠ¤í‹± ì„œì¹˜ì— ì‚½ì…í•  ì˜ˆì •ì¸ë°, jsonìœ¼ë¡œ ì‰½ê²Œ ë³€í™˜í•˜ë ¤ë©´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ í•˜ë¯€ë¡œ `Object.__dict__`ë¡œ Campaign ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.

# ğŸŒ¨ï¸ jsonìœ¼ë¡œ ì €ì¥

```python
# json ìœ¼ë¡œ ì €ì¥
with open('data\happybean.json', "w", encoding = 'utf-8') as f:
    json.dump(data, f, ensure_ascii = False, indent = 4)
```

`list[dict]` í˜•ì‹ìœ¼ë¡œ ì €ì¥ëœ data ë¦¬ìŠ¤íŠ¸ë¥¼ jsonëª¨ë“ˆì„ ì´ìš©í•´ jsoníŒŒì¼ë¡œ ì €ì¥í•´ì¤ë‹ˆë‹¤.

# ğŸ“ƒ ì „ì²´ ì†ŒìŠ¤ì½”ë“œ

<details>
<summary>ì„¸ë¶€í˜ì´ì§€ í¬ë¡¤ë§(ìŠ¤í¬ë˜í•‘)í•˜ëŠ” ì†ŒìŠ¤ì½”ë“œ</summary>
<div markdown="1">
```python
from lib2to3.pgen2 import driver
from selenium import webdriver
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import time
import json

_URL_DONATE_LIST = "https://happybean.naver.com/donation/DonateHomeMain"


class Campaign:
    def __init__(
        self,
        campaign_id,
        title,
        category,
        theme,
        body,
        organization_name,
        thumbnail,
        due_date,
        start_date,
        target_price,
        status_price,
    ):
        self.campaign_id = campaign_id
        self.title = title
        self.category = category
        self.theme = theme
        self.body = body
        self.organization_name = organization_name
        self.thumbnail = thumbnail
        self.due_date = due_date
        self.start_date = start_date
        self.target_price = target_price
        self.status_price = status_price


def get_title(soup: BeautifulSoup):
    return soup.find("h3", "tit").text


def get_theme_and_category(soup: BeautifulSoup):
    _theme = soup.find("a", "theme").text.split(">")
    theme = _theme[1].strip()
    category = _theme[0].strip()
    return theme, category


def get_body(soup: BeautifulSoup):
    body = soup.select_one(
        "#container > div > div.collect_content > div > ul.intro_lst.editor_base"
    ).text.strip()
    return body


def get_organization_name(soup: BeautifulSoup):
    return soup.select_one(
        "#container > div > div.collect_side > div.section_group > div > h3 > span > a > strong"
    ).text.strip()


def get_dates(soup: BeautifulSoup):
    _term = soup.select_one(
        "#container > div > div.collect_side > div.section_status > div.term_area > p > strong"
    ).text.split("~")
    start_date = _term[0].strip()
    due_date = _term[1].strip()
    return start_date, due_date


def get_prices(soup: BeautifulSoup):
    status_price = soup.select_one(
        "#container > div > div.collect_side > div.section_status > div.num_area > p.status_num > strong"
    ).text.replace(",", "")
    target_price = soup.select_one(
        "#container > div > div.collect_side > div.section_status > div.num_area > p.detail_num.v2 > strong > span"
    ).text.replace(",", "")
    return status_price, target_price


def crawling_each_campaign(url: str, src: str):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    campaign_id = url.split("/")[4]
    title = get_title(soup)
    theme, category = get_theme_and_category(soup)
    body = get_body(soup)
    organization_name = get_organization_name(soup)
    thumbnail = src
    start_date, due_date = get_dates(soup)
    status_price, target_price = get_prices(soup)

    campaign = Campaign(
        campaign_id,
        title,
        category,
        theme,
        body,
        organization_name,
        thumbnail,
        due_date,
        start_date,
        target_price,
        status_price,
    )
    data.append(campaign.__dict__)


def set_chrome_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("headless")  # ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ë„ìš°ì§€ ì•ŠëŠ” headless chrome ì˜µì…˜
    options.add_argument("lang=ko_KR")  # ì–¸ì–´ ì„¤ì •
    options.add_experimental_option(
        "excludeSwitches", ["enable-logging"]
    )  # ê°œë°œë„êµ¬ ë¡œê·¸ ìˆ¨ê¸°ê¸°
    options.add_argument("start-maximized")
    options.add_argument("disable-infobars")
    options.add_argument("--disable-extensions")
    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()), options=options
    )
    return driver


if __name__ == "__main__":

    print("... í•´í”¼ë¹ˆ í¬ë¡¤ë§ ì‹œì‘ ...")
    start = time.time()  # ì‹œì‘ì‹œê°„

    data = []

    driver = set_chrome_driver()
    driver.get(_URL_DONATE_LIST)

    # ë”ë³´ê¸° ë²„íŠ¼ ëê¹Œì§€ í´ë¦­
    while driver.find_element(By.CSS_SELECTOR, "#btn_more").is_displayed:
        try:
            WebDriverWait(driver, 1).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "#btn_more"))
            ).click()
        except TimeoutException:
            break

    # ê° ìº í˜ì¸ ì¹´ë“œì— ëŒ€í•œ í¬ë¡¤ë§ ì§„í–‰
    html_campaign_cards = driver.find_elements(By.CLASS_NAME, "card")
    for card in html_campaign_cards:
        campaign_url = card.get_attribute("href")
        campaign_thumbnail = card.find_element(
            By.CSS_SELECTOR, "a > img"
        ).get_attribute("src")
        crawling_each_campaign(campaign_url, campaign_thumbnail)
    driver.close()

    end = time.time()  # ì¢…ë£Œ ì‹œê°„
    print(f"{end - start:.5f} sec")

    print("... í•´í”¼ë¹ˆ í¬ë¡¤ë§ ë ...")

    # json ìœ¼ë¡œ ì €ì¥
    with open("data\happybean.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

```
</div>
</details>

<hr>

ì´ë ‡ê²Œ í•˜ë©´ ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ì— ë„£ì„ ì¬ë£Œ ì¤€ë¹„ëŠ” ëì…ë‹ˆë‹¤ âœ¨

<br>